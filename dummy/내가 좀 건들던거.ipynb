{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To 호현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from konlpy.tag import Kkma, Okt\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 감정 사전 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ksenticnet_kaist import *\n",
    "\n",
    "ksenticnet = get_ksenticnet()\n",
    "\n",
    "keys = list(ksenticnet.keys())\n",
    "senticvals = [[float(i) for i in val[:4]] for val in  ksenticnet.values()]\n",
    "sentiments = []\n",
    "polarity = []\n",
    "semantics = []\n",
    "for key, val in ksenticnet.items():\n",
    "    for i in val[4:]:\n",
    "        if i in ['positive', 'negative']:\n",
    "            polar_ind = val.index(i)\n",
    "            sentiments.append(val[4 : polar_ind])\n",
    "            polarity.append(val[polar_ind : polar_ind+2])\n",
    "            semantics.append(val[polar_ind+2 :])\n",
    "            break\n",
    "\n",
    "ksenticnets = defaultdict(dict)\n",
    "for key, val, senti, p, seman in zip(keys, \n",
    "                                     senticvals, \n",
    "                                     sentiments, \n",
    "                                     polarity, \n",
    "                                     semantics):\n",
    "    ksenticnets[key]['sentic_value'] = val\n",
    "    ksenticnets[key]['sentiment'] = senti\n",
    "    ksenticnets[key]['polarity'] = p\n",
    "    ksenticnets[key]['semantic'] = seman\n",
    "\n",
    "f = lambda x : [i if i > 0 else 0 for i in x]\n",
    "g = lambda x : [abs(i) if i < 0 else 0 for i in x]\n",
    "scores = np.array(list(map(lambda x : f(x) + g(x), senticvals)))\n",
    "scores /= scores.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "class KSenticNet():\n",
    "    keys = {j : i for i, j in  enumerate(keys)}\n",
    "    scores = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentimentLDAGibbsSampler 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 50000\n",
    "\n",
    "def sampleFromDirichlet(alpha):\n",
    "    return np.random.dirichlet(alpha)\n",
    "\n",
    "def sampleFromCategorical(theta):\n",
    "    theta = theta / np.sum(theta)\n",
    "    return np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "def word_indices(wordOccurenceVec):\n",
    "    for idx in wordOccurenceVec.nonzero()[0]:\n",
    "        for i in range(int(wordOccurenceVec[idx])):\n",
    "            yield idx\n",
    "            \n",
    "class KSenticNet():\n",
    "    keys = {j : i for i, j in  enumerate(keys)}\n",
    "    scores = scores\n",
    "    \n",
    "class SentimentLDAGibbsSampler:\n",
    "    \n",
    "    def __init__(self, numTopics, alpha, beta, gamma, numSentiments=2):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.numTopics = numTopics\n",
    "        self.numSentiments = numSentiments\n",
    "        \n",
    "    def processSingleReview(self, review, st, d=None, stopwords=None):\n",
    "        letters_only = re.sub('[^ㄱ-하-ㅣ가-힣]', ' ', review).strip()\n",
    "        if not stopwords:\n",
    "            stops = list('의가이은들는좀잘걍과도를자에와한것') + ['으로', '하다']\n",
    "        else:\n",
    "            stops = stopwords\n",
    "        words = st.morphs(letters_only, stem=True, norm=True)\n",
    "        meaningful_words = [w for w in words if w not in stops]\n",
    "        return ' '.join(meaningful_words)\n",
    "    \n",
    "    def processReviews(self, reviews, st, saveAs=None, saveOverride=False, \n",
    "                       do_preprocess=True, return_processed_review=False):\n",
    "        import os\n",
    "        import dill\n",
    "        if not saveOverride and saveAs and os.path.isfile(saveAs):\n",
    "            [wordOccurenceMatrix, self.vectorizer] = dill.load(open(saveAs, 'r'))\n",
    "            return wordOccurenceMatrix\n",
    "        if do_preprocess:\n",
    "            processed_reviews = []\n",
    "            for i, review in enumerate(reviews):\n",
    "                if (i + 1) % 10000 == 0:\n",
    "                    print(' Review {} of {}'.format(i + 1, len(reviews)))\n",
    "                processed_reviews.append(self.processSingleReview(review, st, i))\n",
    "        else:\n",
    "            processed_reviews = reviews\n",
    "        if return_processed_review:\n",
    "            return processed_reviews\n",
    "        self.vectorizer = CountVectorizer(analyzer='word',\n",
    "                                          tokenizer=None,\n",
    "                                          preprocessor=None,\n",
    "                                          max_features=MAX_VOCAB_SIZE)\n",
    "        train_data_features = self.vectorizer.fit_transform(processed_reviews)\n",
    "        wordOccurenceMatrix = train_data_features\n",
    "        if saveAs:\n",
    "            dill.dump([wordOccurenceMatrix, self.vectorizer], open(saveAs, 'w'))\n",
    "        return wordOccurenceMatrix\n",
    "    \n",
    "    def _initialize_(self, reviews, st, saveAs=None, saveOverride=False, do_preprocess=True):\n",
    "        self.wordOccurenceMatrix = self.processReviews(reviews, st, saveAs, saveOverride, do_preprocess)\n",
    "        numDocs, vocabSize = self.wordOccurenceMatrix.shape\n",
    "        \n",
    "        # Pseudocounts\n",
    "        self.n_dt = np.zeros((numDocs, self.numTopics))\n",
    "        self.n_dts = np.zeros((numDocs, self.numTopics, self.numSentiments))\n",
    "        self.n_d = np.zeros((numDocs))\n",
    "        self.n_vts = np.zeros((vocabSize, self.numTopics, self.numSentiments))\n",
    "        self.n_ts = np.zeros((self.numTopics, self.numSentiments))\n",
    "        self.topics = {}\n",
    "        self.sentiments = {}\n",
    "        self.priorSentiment = {}\n",
    "        \n",
    "        alphaVec = self.alpha * np.ones(self.numTopics)\n",
    "        gammaVec = self.gamma * np.ones(self.numSentiments)\n",
    "        \n",
    "        print('--* KSenticNet으로 사전 확률 조작 중... *--')\n",
    "        # 감정 사전 (KSenticNEt)을 사용하여 사전 확률을 조작 중.\n",
    "        for i, word in enumerate(self.vectorizer.get_feature_names()):\n",
    "            w = KSenticNet.keys.get(word)\n",
    "            if not w: continue\n",
    "            synsets = KSenticNet.scores[w, :]\n",
    "            self.priorSentiment[i] = np.random.choice(self.numSentiments, p=synsets)\n",
    "        \n",
    "        print('--* initialize 작업 진행 중... *--')\n",
    "        for d in range(numDocs):\n",
    "            if d % 5000 == 0: print(' Doc {} of {} Reviews'.format(d, numDocs))\n",
    "            topicDistribution = sampleFromDirichlet(alphaVec)\n",
    "            sentimentDistribution = np.zeros((self.numTopics, self.numSentiments))\n",
    "            for t in range(self.numTopics):\n",
    "                sentimentDistribution[t, :] = sampleFromDirichlet(gammaVec)\n",
    "            for i, w in enumerate(word_indices(self.wordOccurenceMatrix[d, :].toarray()[0])):\n",
    "                t = sampleFromCategorical(topicDistribution)\n",
    "                s = sampleFromCategorical(sentimentDistribution[t, :])\n",
    "                \n",
    "                self.topics[(d, i)] = t\n",
    "                self.sentiments[(d, i)] = s\n",
    "                self.n_dt[d, t] += 1\n",
    "                self.n_dts[d, t, s] += 1\n",
    "                self.n_d[d] += 1\n",
    "                self.n_vts[w, t, s] += 1\n",
    "                self.n_ts[t, s] += 1\n",
    "                \n",
    "    def conditionalDistribution(self, d, v):\n",
    "        probabilites_ts = np.ones((self.numTopics, self.numSentiments))\n",
    "        firstFactor = (self.n_dt[d] + self.alpha) / \\\n",
    "                (self.n_d[d] + self.numTopics * self.alpha)\n",
    "        secondFactor = (self.n_dts[d, :, :] + self.gamma) / \\\n",
    "                (self.n_dt[d, :] + self.numSentiments * self.gamma)[:, np.newaxis]\n",
    "        thirdFactor = (self.n_vts[v, :, :] + self.beta) / \\\n",
    "                (self.n_ts + self.n_vts.shape[0] * self.beta)\n",
    "        probabilites_ts *= firstFactor[:, np.newaxis]\n",
    "        probabilites_ts *= secondFactor * thirdFactor\n",
    "        probabilites_ts /= np.sum(probabilites_ts)\n",
    "        return probabilites_ts\n",
    "                \n",
    "    def run(self, reviews, st, maxIters=30, saveAs=None, saveOverride=False, do_preprocess=True):\n",
    "        self._initialize_(reviews, st, saveAs, saveOverride, do_preprocess)\n",
    "        numDocs, vocabSize = self.wordOccurenceMatrix.shape\n",
    "        for iteration in range(maxIters):\n",
    "            gc.collect()\n",
    "            print('Starting iteration {} of {}'.format(iteration + 1, maxIters))\n",
    "            for d in range(numDocs):\n",
    "                for i, v in enumerate(word_indices(self.wordOccurenceMatrix[d, :].toarray()[0])):\n",
    "                    t = self.topics[(d, i)]\n",
    "                    s = self.sentiments[(d, i)]\n",
    "                    self.n_dt[d, t] -= 1\n",
    "                    self.n_d[d] -= 1\n",
    "                    self.n_dts[d, t, s] -= 1\n",
    "                    self.n_vts[v, t, s] -= 1\n",
    "                    self.n_ts[t, s] -= 1\n",
    "                    \n",
    "                    probabilites_ts = self.conditionalDistribution(d, v)\n",
    "                    if v in self.priorSentiment:\n",
    "                        s = self.priorSentiment[v]\n",
    "                        t = sampleFromCategorical(probabilites_ts[:, s])\n",
    "                    else:\n",
    "                        ind = sampleFromCategorical(probabilites_ts.flatten())\n",
    "                        t, s = np.unravel_index(ind, probabilites_ts.shape)\n",
    "                    \n",
    "                    self.topics[(d, i)] = t\n",
    "                    self.sentiments[(d, i)] = s\n",
    "                    self.n_dt[d, t] += 1\n",
    "                    self.n_d[d] += 1\n",
    "                    self.n_dts[d, t, s] += 1\n",
    "                    self.n_vts[v, t, s] += 1\n",
    "                    self.n_ts[t, s] += 1\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 호출 (spacing 전처리 수행 o)\n",
    "df2 = pd.read_csv('spacing_nsmc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 분류 호출\n",
    "with open('1st_jst_result.pkl', 'rb') as f:\n",
    "    JST = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "for i, j in JST.sentiments.items():\n",
    "    res[i[0]].append(j)\n",
    "\n",
    "from collections import Counter\n",
    "res = {i : Counter(j) for i, j in res.items()}\n",
    "\n",
    "i2senti = {0 : 'joy', 1 : 'interest', 2 : 'anger', 3 : 'admiration',\n",
    "           4 : 'sadness', 5 : 'surprise', 6 : 'fear', 7 : 'disgust'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_label_each_review = [[] for _ in range(len(df2))]\n",
    "for i in range(len(df2)):\n",
    "    if res.get(i):\n",
    "        for j in res.get(i).most_common(2):\n",
    "            senti_label_each_review[i].append(i2senti[j[0]])\n",
    "    else:\n",
    "        senti_label_each_review[i].append(['neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df2['review'].tolist()\n",
    "y_train = senti_label_each_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['전체 관람가는 아닌 것 같아요',\n",
       " '디렉터스 컷으로 봐서 거의 3시간 짜리인데 참 흡인력 있다',\n",
       " '태어나 처음으로 가슴 아리는 영화였다. 20년 이상 지났지만.. 생각하면 또 가슴이 아리는.. 황순원의 소나기에서 또 한번 느꼈던 그 느낌!',\n",
       " '어린시절 고딩 때 봤던 때랑 또 결혼하고 나서 봤을 때의 느낌은 확실히 다르네요. 뭔가 알프레도를 더 이해하게 되고.. 토토와 알프레도의 우정이 정말 아름다운 것이었음을, 토토의 첫사랑이 참 풋풋했음을 느낍니다~~ 그리고 언제 들어도 사랑스러운 최고의 영화음악!',\n",
       " '토토에게 넓은 세상을 보여주고 픈 알프레도.. 그가 토토를 위해 정을 떼려고 했던 장면에 왠지 씁쓸했고, 친구, 스승을 떠나 아버지 같은 느낌을 받게 되었다.. 평생 못잊을 장면이자 추억이다.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fear', 'admiration'],\n",
       " ['surprise', 'sadness'],\n",
       " ['admiration', 'sadness'],\n",
       " ['admiration', 'interest'],\n",
       " ['joy', 'interest']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = tokenizer_twitter_morphs(X_train[i])\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래에 분류 모델 만들어 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-embedding: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, f1_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "from time import time\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "def tokenizer_okt_morphs(doc):\n",
    "    return okt.morphs(doc)\n",
    "\n",
    "def tokenizer_okt_noun(doc):\n",
    "    return okt.nouns(doc)\n",
    "\n",
    "def tokenizer_okt_pos(doc):\n",
    "    return okt.pos(doc, norm=True, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()\n",
    "def tokenizer_noun(doc):\n",
    "    return komoran.nouns(doc)\n",
    "\n",
    "def tokenizer_morphs(doc):\n",
    "    return komoran.morphs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for line in X_train:\n",
    "    malist = okt.pos(line, norm = True, stem = True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        if not word[1] in [\"Josa\",\"Eomi\",\"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    result.append(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['전체 관람 가다 아니다 것 같다',\n",
       " '디렉터 스 컷 보다 거의 3시간 짜다 리 차다 흡인 력 있다',\n",
       " '태어나다 처음 가슴 아리다 영화 이다 20년 이상 지나다 생각 하다 또 가슴 아리다 황순원 소나기 또 한번 느끼다 그 느낌',\n",
       " '어린시절 고딩 때 보다 때 또 결혼 나서다 보다 때 느낌 확실하다 다르다 뭔가 알 프레 도르다 더 이해 하다 되다 토토 알 프레 도의 우정 정말 아름답다 것 이다 음 토토 첫사랑 차다 풋풋하다 음 느끼다 그리고 언제 들다 사랑스럽다 최고 영화음악',\n",
       " '토토 넓다 세상 보여주다 픈 알 프레 그 토토 위해 정 떼다 하다 장면 왠지 씁쓸하다 친구 스승 떠나다 아버지 같다 느낌 받다 되어다 평생 못 잊다 장면 이자 추억']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['전체', '관람', '가다', '아니다', '것', '같다'],\n",
       " ['디렉터', '스', '컷', '보다', '거의', '3시간', '짜다', '리', '차다', '흡인', '력', '있다'],\n",
       " ['태어나다',\n",
       "  '처음',\n",
       "  '가슴',\n",
       "  '아리',\n",
       "  '다',\n",
       "  '영화',\n",
       "  '이다',\n",
       "  '20년',\n",
       "  '이상',\n",
       "  '지나다',\n",
       "  '생각',\n",
       "  '하다',\n",
       "  '또',\n",
       "  '가슴',\n",
       "  '아리',\n",
       "  '다',\n",
       "  '황순원',\n",
       "  '소나기',\n",
       "  '또',\n",
       "  '한번',\n",
       "  '느끼다',\n",
       "  '그',\n",
       "  '느낌'],\n",
       " ['어린시절',\n",
       "  '고딩',\n",
       "  '때',\n",
       "  '보다',\n",
       "  '때',\n",
       "  '또',\n",
       "  '결혼',\n",
       "  '나서다',\n",
       "  '보다',\n",
       "  '때',\n",
       "  '느낌',\n",
       "  '확실하다',\n",
       "  '다르다',\n",
       "  '뭔가',\n",
       "  '알',\n",
       "  '프레',\n",
       "  '도르다',\n",
       "  '더',\n",
       "  '이해',\n",
       "  '하다',\n",
       "  '되다',\n",
       "  '토토',\n",
       "  '알',\n",
       "  '프레',\n",
       "  '도의',\n",
       "  '우정',\n",
       "  '정말',\n",
       "  '아름답다',\n",
       "  '것',\n",
       "  '이다',\n",
       "  '음',\n",
       "  '토토',\n",
       "  '첫사랑',\n",
       "  '차다',\n",
       "  '풋풋하다',\n",
       "  '음',\n",
       "  '느끼다',\n",
       "  '그리고',\n",
       "  '언제',\n",
       "  '들다',\n",
       "  '사랑스럽다',\n",
       "  '최고',\n",
       "  '영화음악'],\n",
       " ['토토',\n",
       "  '넓다',\n",
       "  '세상',\n",
       "  '보여주다',\n",
       "  '픈',\n",
       "  '알',\n",
       "  '프레',\n",
       "  '그',\n",
       "  '토토',\n",
       "  '위해',\n",
       "  '정',\n",
       "  '떼다',\n",
       "  '하다',\n",
       "  '장면',\n",
       "  '왠지',\n",
       "  '씁쓸하다',\n",
       "  '친구',\n",
       "  '스승',\n",
       "  '떠나다',\n",
       "  '아버지',\n",
       "  '같다',\n",
       "  '느낌',\n",
       "  '받다',\n",
       "  '되어다',\n",
       "  '평생',\n",
       "  '못',\n",
       "  '잊다',\n",
       "  '장면',\n",
       "  '이자',\n",
       "  '추억']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(X_train_t)):\n",
    "    X_train_t[i] = tokenizer_okt_morphs(X_train_t[i])\n",
    "X_train_t[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306743 306743\n",
      "[['디렉터', '스', '컷', '보다', '거의', '3시간', '짜다', '리', '차다', '흡인', '력', '있다'], ['태어나다', '처음', '가슴', '아리', '다', '영화', '이다', '20년', '이상', '지나다', '생각', '하다', '또', '가슴', '아리', '다', '황순원', '소나기', '또', '한번', '느끼다', '그', '느낌'], ['어린시절', '고딩', '때', '보다', '때', '또', '결혼', '나서다', '보다', '때', '느낌', '확실하다', '다르다', '뭔가', '알', '프레', '도르다', '더', '이해', '하다', '되다', '토토', '알', '프레', '도의', '우정', '정말', '아름답다', '것', '이다', '음', '토토', '첫사랑', '차다', '풋풋하다', '음', '느끼다', '그리고', '언제', '들다', '사랑스럽다', '최고', '영화음악'], ['토토', '넓다', '세상', '보여주다', '픈', '알', '프레', '그', '토토', '위해', '정', '떼다', '하다', '장면', '왠지', '씁쓸하다', '친구', '스승', '떠나다', '아버지', '같다', '느낌', '받다', '되어다', '평생', '못', '잊다', '장면', '이자', '추억'], ['인생', '최고', '영화', '말', '필요', '없다', '감독판', '감동', '좀', '덜하다']]\n"
     ]
    }
   ],
   "source": [
    "X_train_t_10 = []\n",
    "y_train_t_10 = []\n",
    "for i in range(len(X_train_t)):\n",
    "    if len(X_train_t[i]) >= 10:\n",
    "        X_train_t_10.append(result[i])\n",
    "        y_train_t_10.append(y_train[i])\n",
    "print(len(X_train_t_10),len(y_train_t_10))\n",
    "print(X_train_t_10[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_c[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenizer_okt_morphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer():\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian_nbc = Pipeline([('vect', tfidf), ('nbc', GaussianNB())], ('to_dense', DenseTransformer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'memory' should be None, a string or have the same interface as joblib.Memory. Got memory='('to_dense', <__main__.DenseTransformer object at 0x0000023BB06FA048>)' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-7a540c23c331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mGaussian_nbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_t_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_t_10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time: {:f}s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# Setup the memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mfit_transform_one_cached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_fit_transform_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_memory\u001b[1;34m(memory)\u001b[0m\n\u001b[0;32m    184\u001b[0m         raise ValueError(\"'memory' should be None, a string or have the same\"\n\u001b[0;32m    185\u001b[0m                          \u001b[1;34m\" interface as joblib.Memory.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                          \" Got memory='{}' instead.\".format(memory))\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'memory' should be None, a string or have the same interface as joblib.Memory. Got memory='('to_dense', <__main__.DenseTransformer object at 0x0000023BB06FA048>)' instead."
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "Gaussian_nbc.fit(X_train_t_10, y_train_t_10)\n",
    "end = time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = Gaussian_nbc.predict(test[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No matching overloads found for kr.lucypark.okt.OktInterface.tokenize(list,java.lang.Boolean,java.lang.Boolean), options are:\n\tpublic java.util.List kr.lucypark.okt.OktInterface.tokenize(java.lang.String,java.lang.Boolean,java.lang.Boolean)\n\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:242)\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:245)\n\tat JPMethod::invoke(native\\common\\jp_method.cpp:253)\n\tat PyJPMethod::__call__(native\\python\\pyjp_method.cpp:142)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-954926cff504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_twitter_morphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-276b6d39fb3f>\u001b[0m in \u001b[0;36mtokenizer_twitter_morphs\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtwitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer_twitter_morphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenizer_twitter_noun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mmorphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mphrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\konlpy\\tag\\_okt.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No matching overloads found for kr.lucypark.okt.OktInterface.tokenize(list,java.lang.Boolean,java.lang.Boolean), options are:\n\tpublic java.util.List kr.lucypark.okt.OktInterface.tokenize(java.lang.String,java.lang.Boolean,java.lang.Boolean)\n\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:242)\n\tat JPMethod::findOverload(native\\common\\jp_method.cpp:245)\n\tat JPMethod::invoke(native\\common\\jp_method.cpp:253)\n\tat PyJPMethod::__call__(native\\python\\pyjp_method.cpp:142)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = tokenizer_twitter_morphs(X_train[i])\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_nb, tpr_nb, thresholds = metrics.roc_curve(test[\"label\"], y_pred_nb)\n",
    "print(\"Test Accuracy  : {:.3f}\".format(accuracy_score(test[\"label\"], y_pred_nb)))\n",
    "print(\"Test Precision : {:.3f}\".format(precision_score(test[\"label\"], y_pred_nb)))\n",
    "print(\"Test Recall    : {:.3f}\".format(recall_score(test[\"label\"], y_pred_nb)))\n",
    "print(\"Test F1_score  : {:.3f}\".format(f1_score(test[\"label\"], y_pred_nb)))\n",
    "print(\"Test auc       : {:.3f}\".format(metrics.auc(fpr_nb, tpr_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndf_clf = Pipeline([('vect', tfidf), ('rndf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "rndf_clf.fit(X_train, y_train)\n",
    "end = time()\n",
    "print('Time: {:f}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rndf = rndf_clf.predict(test[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rndf, tpr_rndf, thresholds = metrics.roc_curve(test[\"label\"], y_pred_rndf)\n",
    "print(\"Test Accuracy  : {:.3f}\".format(accuracy_score(test[\"label\"], y_pred_rndf)))\n",
    "print(\"Test Precision : {:.3f}\".format(precision_score(test[\"label\"], y_pred_rndf)))\n",
    "print(\"Test Recall    : {:.3f}\".format(recall_score(test[\"label\"], y_pred_rndf)))\n",
    "print(\"Test F1_score  : {:.3f}\".format(f1_score(test[\"label\"], y_pred_rndf)))\n",
    "print(\"Test auc       : {:.3f}\".format(metrics.auc(fpr_sgd, tpr_rndf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
